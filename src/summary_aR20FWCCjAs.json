{
  "video_id": "aR20FWCCjAs",
  "url": "https://www.youtube.com/watch?v=aR20FWCCjAs",
  "transcript": "You know what's crazy? That all of this is real.\nMeaning what?  Don't you think so? All this AI stuff and \nall this Bay Area… that it's happening.  Isn't it straight out of science fiction?\nAnother thing that's crazy is how   normal the slow takeoff feels.\nThe idea that we'd be investing 1%   of GDP in AI, I feel like it would have felt like \na bigger deal, whereas right now it just feels...  We get used to things pretty fast, it turns out.\nBut also it's kind of abstract. What does it   mean? It means that you see it in the news, \nthat such and such company announced such   and such dollar amount. That's all you see. \nIt's not really felt in any other way so far.  Should we actually begin here? I think \nthis is an interesting discussion.  Sure.\nI think your point,   about how from the average person's point of view \nnothing is that different, will continue being   true even into the singularity.\nNo, I don't think so.  Okay, interesting.\nThe thing which I was referring   to not feeling different is, okay, such and such \ncompany announced some difficult-to-comprehend   dollar amount of investment.\nI don't think anyone knows what to do with that.  But I think the impact of AI is going to be felt.\nAI is going to be diffused through the economy.  There'll be very strong economic forces \nfor this, and I think the impact is   going to be felt very strongly.\nWhen do you expect that impact?  I think the models seem smarter than \ntheir economic impact would imply.  Yeah. This is one of the very confusing \nthings about the models right now.  How to reconcile the fact that \nthey are doing so well on evals?  You look at the evals and you go, \"Those \nare pretty hard evals.\" They are doing   so well. But the economic impact \nseems to be dramatically behind.  It's very difficult to make sense of, \nhow can the model, on the one hand,   do these amazing things, and then on the other \nhand, repeat itself twice in some situation?  An example would be, let's say you \nuse vibe coding to do something.  You go to some place and then you get a bug.\nThen you tell the model,   \"Can you please fix the bug?\"\nAnd the model says, \"Oh my God,   you're so right. I have a bug. Let me go \nfix that.\" And it introduces a second bug.  Then you tell it, \"You have this \nnew second bug,\" and it tells you,   \"Oh my God, how could I have done it?\nYou're so right again,\" and brings back   the first bug, and you can alternate between \nthose. How is that possible? I'm not sure, but it   does suggest that something strange is going on. I \nhave two possible explanations. The more whimsical   explanation is that maybe RL training makes the \nmodels a little too single-minded and narrowly   focused, a little bit too unaware, even though \nit also makes them aware in some other ways.  Because of this, they can't do basic things. \nBut there is another explanation. Back when   people were doing pre-training, the \nquestion of what data to train on was   answered, because that answer was everything.\nWhen you do pre-training, you need all the data.  So you don't have to think if it's \ngoing to be this data or that data.  But when people do RL training, \nthey do need to think.  They say, \"Okay, we want to have this \nkind of RL training for this thing   and that kind of RL training for that thing.\"\nFrom what I hear, all the companies have teams   that just produce new RL environments \nand just add it to the training mix.  The question is, well, what are those?\nThere are so many degrees of freedom.  There is such a huge variety of \nRL environments you could produce.  One thing you could do, and I think this \nis something that is done inadvertently,   is that people take inspiration from the evals.\nYou say, \"Hey, I would love our model to do   really well when we release it.\nI want the evals to look great.  What would be RL training \nthat could help on this task?\"  I think that is something that happens, and \nit could explain a lot of what's going on.  If you combine this with generalization \nof the models actually being inadequate,   that has the potential to explain a lot \nof what we are seeing, this disconnect   between eval performance and actual real-world \nperformance, which is something that we don't   today even understand, what we mean by that.\nI like this idea that the real reward hacking   is the human researchers who \nare too focused on the evals.  I think there are two ways to \nunderstand, or to try to think about,   what you have just pointed out.\nOne is that if it's the case that   simply by becoming superhuman at a coding \ncompetition, a model will not automatically   become more tasteful and exercise better judgment \nabout how to improve your codebase, well then you   should expand the suite of environments such \nthat you're not just testing it on having   the best performance in coding competition.\nIt should also be able to make the best kind   of application for X thing or Y thing or Z thing.\nAnother, maybe this is what you're hinting at,   is to say, \"Why should it be the case in \nthe first place that becoming superhuman   at coding competitions doesn't make you a \nmore tasteful programmer more generally?\"  Maybe the thing to do is not to keep \nstacking up the amount and diversity   of environments, but to figure out an approach \nwhich lets you learn from one environment and   improve your performance on something else.\nI have a human analogy which might be helpful.  Let's take the case of competitive programming, \nsince you mentioned that. Suppose you have two   students. One of them decided they want \nto be the best competitive programmer, so   they will practice 10,000 hours for that domain.\nThey will solve all the problems, memorize all the   proof techniques, and be very skilled at quickly \nand correctly implementing all the algorithms.  By doing so, they became one of the best.\nStudent number two thought, \"Oh,   competitive programming is cool.\"\nMaybe they practiced for 100 hours,   much less, and they also did really well.\nWhich one do you think is going to do better   in their career later on?\nThe second.  Right. I think that's basically what's going on.\nThe models are much more like the   first student, but even more.\nBecause then we say, the model should   be good at competitive programming so let's get \nevery single competitive programming problem ever.  And then let's do some data augmentation \nso we have even more competitive   programming problems, and we train on that.\nNow you've got this great competitive programmer.  With this analogy, I think it's more intuitive.\nYeah, okay, if it's so well trained, all the   different algorithms and all the different \nproof techniques are right at its fingertips.  And it's more intuitive that with this \nlevel of preparation, it would not   necessarily generalize to other things.\nBut then what is the analogy for what   the second student is doing before \nthey do the 100 hours of fine-tuning?  I think they have \"it.\" The \"it\" \nfactor. When I was an undergrad,   I remember there was a student like this \nthat studied with me, so I know it exists.  I think it's interesting to distinguish \n\"it\" from whatever pre-training does.  One way to understand what you just said \nabout not having to choose the data in   pre-training is to say it's actually not \ndissimilar to the 10,000 hours of practice.  It's just that you get that 10,000 hours \nof practice for free because it's already   somewhere in the pre-training distribution.\nBut maybe you're suggesting there's actually   not that much generalization from pre-training.\nThere's just so much data in pre-training, but   it's not necessarily generalizing better than RL.\nThe main strength of pre-training is   that: A, there is so much of it, and B, \nyou don't have to think hard about what   data to put into pre-training.\nIt's very natural data, and it   does include in it a lot of what people do: \npeople's thoughts and a lot of the features.  It's like the whole world as projected by \npeople onto text, and pre-training tries   to capture that using a huge amount of data.\nPre-training is very difficult to reason about   because it's so hard to understand the manner \nin which the model relies on pre-training data.  Whenever the model makes a mistake, could it be \nbecause something by chance is not as supported   by the pre-training data? \"Support by \npre-training\" is maybe a loose term.  I don't know if I can add \nanything more useful on this.  I don't think there is a \nhuman analog to pre-training.  Here are analogies that people have proposed \nfor what the human analogy to pre-training is.  I'm curious to get your thoughts \non why they're potentially wrong.  One is to think about the first 18, or 15, \nor 13 years of a person's life when they   aren't necessarily economically productive, \nbut they are doing something that is making   them understand the world better and so forth.\nThe other is to think about evolution as doing   some kind of search for 3 billion years, which \nthen results in a human lifetime instance.  I'm curious if you think either of \nthese are analogous to pre-training.  How would you think about what lifetime \nhuman learning is like, if not pre-training?  I think there are some similarities between both \nof these and pre-training, and pre-training tries   to play the role of both of these.\nBut I think there are some   big differences as well.\nThe amount of pre-training data is very,   very staggering.\nYes.  Somehow a human being, after even 15 years \nwith a tiny fraction of the pre-training   data, they know much less.\nBut whatever they do know,   they know much more deeply somehow.\nAlready at that age, you would not   make mistakes that our AIs make. There is another \nthing. You might say, could it be something like   evolution? The answer is maybe. But in this case, \nI think evolution might actually have an edge.  I remember reading about this case.\nOne way in which neuroscientists can   learn about the brain is by studying people with \nbrain damage to different parts of the brain.  Some people have the most strange symptoms \nyou could imagine. It's actually really,   really interesting. One case that \ncomes to mind that's relevant.  I read about this person who had some kind \nof brain damage, a stroke or an accident,   that took out his emotional processing.\nSo he stopped feeling any emotion.  He still remained very articulate \nand he could solve little puzzles,   and on tests he seemed to be just fine. \nBut he felt no emotion. He didn't feel sad,   he didn't feel anger, he didn't feel animated.\nHe became somehow extremely bad at making any   decisions at all.\nIt would take him   hours to decide on which socks to wear.\nHe would make very bad financial decisions.  What does it say about the role of our built-in \nemotions in making us a viable agent, essentially?  To connect to your question about pre-training, \nmaybe if you are good enough at getting everything   out of pre-training, you could get that as well.\nBut that's the kind of thing which seems...  Well, it may or may not be possible \nto get that from pre-training.  What is \"that\"? Clearly not just directly \nemotion. It seems like some almost value   function-like thing which is telling you what \nthe end reward for any decision should be.  You think that doesn't sort of \nimplicitly come from pre-training?  I think it could. I'm just \nsaying it's not 100% obvious.  But what is that? How do you think about emotions?\nWhat is the ML analogy for emotions?  It should be some kind of a value function thing.\nBut I don’t think there is a great ML analogy   because right now, value functions don't play \na very prominent role in the things people do.  It might be worth defining for the audience what \na value function is, if you want to do that.  Certainly, I'll be very happy to do that.\nWhen people do reinforcement learning,   the way reinforcement learning is done \nright now, how do people train those agents?  You have your neural net and you \ngive it a problem, and then you   tell the model, \"Go solve it.\"\nThe model takes maybe thousands,   hundreds of thousands of actions or thoughts or \nsomething, and then it produces a solution. The   solution is graded. And then the score \nis used to provide a training signal   for every single action in your trajectory.\nThat means that if you are doing something   that goes for a long time—if you're training \na task that takes a long time to solve—it   will do no learning at all until you \ncome up with the proposed solution.  That's how reinforcement learning is done naively.\nThat's how o1, R1 ostensibly are done.  The value function says something like, \n\"Maybe I could sometimes, not always,   tell you if you are doing well or badly.\"\nThe notion of a value function is more   useful in some domains than others.\nFor example, when you play chess and   you lose a piece, I messed up.\nYou don't need to play the whole   game to know that what I just did was bad, and \ntherefore whatever preceded it was also bad.  The value function lets you short-circuit \nthe wait until the very end.  Let's suppose that you are doing some kind \nof a math thing or a programming thing,   and you're trying to explore a \nparticular solution or direction.  After, let's say, a thousand steps of thinking, \nyou concluded that this direction is unpromising.  As soon as you conclude this, you \ncould already get a reward signal   a thousand timesteps previously, when \nyou decided to pursue down this path.  You say, \"Next time I shouldn't pursue this \npath in a similar situation,\" long before you   actually came up with the proposed solution.\nThis was in the DeepSeek R1 paper— that the   space of trajectories is so wide that \nmaybe it's hard to learn a mapping   from an intermediate trajectory and value.\nAnd also given that, in coding for example   you'll have the wrong idea, then you'll \ngo back, then you'll change something.  This sounds like such lack \nof faith in deep learning.  Sure it might be difficult, but \nnothing deep learning can't do.  My expectation is that a value function should \nbe useful, and I fully expect that they will   be used in the future, if not already.\nWhat I was alluding to with the person   whose emotional center got damaged, it’s more \nthat maybe what it suggests is that the value   function of humans is modulated by emotions in \nsome important way that's hardcoded by evolution.  And maybe that is important for \npeople to be effective in the world.  That's the thing I was planning on asking you.  There's something really interesting about \nemotions of the value function, which is that   it's impressive that they have this much utility \nwhile still being rather simple to understand.  I have two responses. I do agree that compared to \nthe kind of things that we learn and the things   we are talking about, the kind of AI we are \ntalking about, emotions are relatively simple.  They might even be so simple that maybe you \ncould map them out in a human-understandable way.  I think it would be cool to do.\nIn terms of utility though,   I think there is a thing where there is this \ncomplexity-robustness tradeoff, where complex   things can be very useful, but simple things are \nvery useful in a very broad range of situations.  One way to interpret what we are seeing is that \nwe've got these emotions that evolved mostly   from our mammal ancestors and then fine-tuned a \nlittle bit while we were hominids, just a bit.  We do have a decent amount of social emotions \nthough which mammals may lack. But they're   not very sophisticated. And because they're \nnot sophisticated, they serve us so well in   this very different world compared to the \none that we've been living in. Actually,   they also make mistakes. For example, our \nemotions… Well actually, I don’t know.  Does hunger count as an emotion? It's debatable. \nBut I think, for example, our intuitive feeling   of hunger is not succeeding in guiding us \ncorrectly in this world with an abundance of food.  People have been talking about scaling \ndata, scaling parameters, scaling compute.  Is there a more general \nway to think about scaling?  What are the other scaling axes?\nHere's a perspective that I think might be true.  The way ML used to work is that \npeople would just tinker with   stuff and try to get interesting results.\nThat's what's been going on in the past. Then   the scaling insight arrived. Scaling laws, GPT-3, \nand suddenly everyone realized we should scale.  This is an example of how language \naffects thought. \"Scaling\" is just   one word, but it's such a powerful word \nbecause it informs people what to do.  They say, \"Let's try to scale things.\"\nSo you say, what are we scaling?  Pre-training was the thing to scale.\nIt was a particular scaling recipe.  The big breakthrough of pre-training is \nthe realization that this recipe is good.  You say, \"Hey, if you mix some compute \nwith some data into a neural net of   a certain size, you will get results.\nYou will know that you'll be better if you   just scale the recipe up.\" This is also great. \nCompanies love this because it gives you a very   low-risk way of investing your resources.\nIt's much harder to invest your resources   in research. Compare that. If you research, \nyou need to be like, \"Go forth researchers   and research and come up with something\", \nversus get more data, get more compute.  You know you'll get something from pre-training.\nIndeed, it looks like, based on various   things some people say on Twitter, maybe it \nappears that Gemini have found a way to get   more out of pre-training.\nAt some point though,   pre-training will run out of data.\nThe data is very clearly finite. What   do you do next? Either you do some kind \nof souped-up pre-training, a different   recipe from the one you've done before, or \nyou're doing RL, or maybe something else.  But now that compute is big, compute \nis now very big, in some sense we   are back to the age of research.\nMaybe here's another way to put it.  Up until 2020, from 2012 to \n2020, it was the age of research.  Now, from 2020 to 2025, it was the \nage of scaling—maybe plus or minus,   let's add error bars to those years—because \npeople say, \"This is amazing. You've got to   scale more. Keep scaling.\" The one word: \nscaling. But now the scale is so big.  Is the belief really, \"Oh, it's so big, but if you \nhad 100x more, everything would be so different?\"  It would be different, for sure.\nBut is the belief that if you just   100x the scale, everything would be transformed? \nI don't think that's true. So it's back to the age   of research again, just with big computers.\nThat's a very interesting way to put it.  But let me ask you the \nquestion you just posed then.  What are we scaling, and what \nwould it mean to have a recipe?  I guess I'm not aware of a very clean \nrelationship that almost looks like a law   of physics which existed in pre-training.\nThere was a power law between data or   compute or parameters and loss.\nWhat is the kind of relationship   we should be seeking, and how should we think \nabout what this new recipe might look like?  We've already witnessed a transition from one \ntype of scaling to a different type of scaling,   from pre-training to RL. Now people are scaling \nRL. Now based on what people say on Twitter,   they spend more compute on RL than on \npre-training at this point, because RL   can actually consume quite a bit of compute.\nYou do very long rollouts, so it takes a lot   of compute to produce those rollouts.\nThen you get a relatively small amount   of learning per rollout, so you \nreally can spend a lot of compute.  I wouldn't even call it scaling.\nI would say, \"Hey, what are you doing?  Is the thing you are doing the most \nproductive thing you could be doing?  Can you find a more productive \nway of using your compute?\"  We've discussed the value \nfunction business earlier.  Maybe once people get good at value \nfunctions, they will be using their   resources more productively.\nIf you find a whole other way   of training models, you could say, \"Is this \nscaling or is it just using your resources?\"  I think it becomes a little bit ambiguous.\nIn the sense that, when people were in the   age of research back then, it was, \n\"Let's try this and this and this.  Let's try that and that and that.\nOh, look, something interesting is happening.\"  I think there will be a return to that.\nIf we're back in the era of research,   stepping back, what is the part of the \nrecipe that we need to think most about?  When you say value function, people \nare already trying the current recipe,   but then having LLM-as-a-Judge and so forth.\nYou could say that's a value function,   but it sounds like you have something \nmuch more fundamental in mind.  Should we even rethink pre-training at all and not \njust add more steps to the end of that process?  The discussion about value function, \nI think it was interesting.  I want to emphasize that I think the value \nfunction is something that's going to make RL more   efficient, and I think that makes a difference.\nBut I think anything you can do with a value   function, you can do without, just more slowly.\nThe thing which I think is the most fundamental   is that these models somehow just generalize \ndramatically worse than people. It's super   obvious. That seems like a very fundamental thing.\nSo this is the crux: generalization. There are two   sub-questions. There's one which is about sample \nefficiency: why should it take so much more data   for these models to learn than humans? There's \na second question. Even separate from the amount   of data it takes, why is it so hard to teach \nthe thing we want to a model than to a human?  For a human, we don't necessarily need a \nverifiable reward to be able to… You're probably   mentoring a bunch of researchers right now, and \nyou're talking with them, you're showing them your   code, and you're showing them how you think.\nFrom that, they're picking up your way of   thinking and how they should do research.\nYou don’t have to set a verifiable reward for   them that's like, \"Okay, this is the next part of \nthe curriculum, and now this is the next part of   your curriculum. Oh, this training was unstable.\" \nThere's not this schleppy, bespoke process.  Perhaps these two issues are actually \nrelated in some way, but I'd be curious   to explore this second thing, which is more \nlike continual learning, and this first thing,   which feels just like sample efficiency.\nYou could actually wonder that one possible   explanation for the human sample efficiency \nthat needs to be considered is evolution.  Evolution has given us a small amount \nof the most useful information possible.  For things like vision, hearing, and \nlocomotion, I think there's a pretty   strong case that evolution has given us a lot.\nFor example, human dexterity far exceeds… I mean   robots can become dexterous too if you subject \nthem to a huge amount of training in simulation.  But to train a robot in the real world \nto quickly pick up a new skill like   a person does seems very out of reach.\nHere you could say, \"Oh yeah, locomotion.  All our ancestors needed \ngreat locomotion, squirrels.  So with locomotion, maybe we've \ngot some unbelievable prior.\"  You could make the same case for vision.\nI believe Yann LeCun made the point that   children learn to drive after 10 \nhours of practice, which is true.  But our vision is so good.\nAt least for me,   I remember myself being a five-year-old.\nI was very excited about cars back then.  I'm pretty sure my car recognition was more than \nadequate for driving already as a five-year-old.  You don't get to see that \nmuch data as a five-year-old.  You spend most of your time in your parents' \nhouse, so you have very low data diversity.  But you could say maybe that's evolution too.\nBut in language and math and coding, probably not.  It still seems better than models.\nObviously, models are better than the average   human at language, math, and coding.\nBut are they better than   the average human at learning?\nOh yeah. Oh yeah, absolutely. What I meant   to say is that language, math, and coding—and \nespecially math and coding—suggests that whatever   it is that makes people good at learning is \nprobably not so much a complicated prior,   but something more, some fundamental thing.\nI'm not sure I understood. Why   should that be the case?\nSo consider a skill in which   people exhibit some kind of great reliability.\nIf the skill is one that was very useful to our   ancestors for many millions of years, hundreds \nof millions of years, you could argue that maybe   humans are good at it because of evolution, \nbecause we have a prior, an evolutionary prior   that's encoded in some very non-obvious \nway that somehow makes us so good at it.  But if people exhibit great ability, reliability, \nrobustness, and ability to learn in a domain that   really did not exist until recently, then \nthis is more an indication that people   might have just better machine learning, period.\nHow should we think about what that is? What is   the ML analogy? There are a couple of interesting \nthings about it. It takes fewer samples. It's   more unsupervised. A child learning to drive a \ncar… Children are not learning to drive a car.  A teenager learning how to drive a car is not \nexactly getting some prebuilt, verifiable reward.  It comes from their interaction with \nthe machine and with the environment.   It takes much fewer samples. It seems \nmore unsupervised. It seems more robust?  Much more robust. The robustness \nof people is really staggering.  Do you have a unified way of thinking about \nwhy all these things are happening at once?  What is the ML analogy that could \nrealize something like this?  One of the things that you've been asking about is \nhow can the teenage driver self-correct and learn   from their experience without an external teacher?\nThe answer is that they have their value function.  They have a general sense which is also, \nby the way, extremely robust in people.  Whatever the human value function is, \nwith a few exceptions around addiction,   it's actually very, very robust.\nSo for something like a teenager   that's learning to drive, they start to drive, and \nthey already have a sense of how they're driving   immediately, how badly they are, how unconfident. \nAnd then they see, \"Okay.\" And then, of course,   the learning speed of any teenager is so fast.\nAfter 10 hours, you're good to go.  It seems like humans have some \nsolution, but I'm curious about   how they are doing it and why is it so hard?\nHow do we need to reconceptualize the way   we're training models to make \nsomething like this possible?  That is a great question to ask, and it's \na question I have a lot of opinions about.  But unfortunately, we live in a world where \nnot all machine learning ideas are discussed   freely, and this is one of them.\nThere's probably a way to do it.  I think it can be done.\nThe fact that people are like that,   I think it's a proof that it can be done.\nThere may be another blocker though,   which is that there is a possibility that the \nhuman neurons do more compute than we think.  If that is true, and if that plays an important \nrole, then things might be more difficult.  But regardless, I do think it points to \nthe existence of some machine learning   principle that I have opinions on.\nBut unfortunately, circumstances   make it hard to discuss in detail.\nNobody listens to this podcast, Ilya.  I'm curious. If you say we are back in an era \nof research, you were there from 2012 to 2020.  What is the vibe now going to be if \nwe go back to the era of research?  For example, even after AlexNet, the \namount of compute that was used to   run experiments kept increasing, and the \nsize of frontier systems kept increasing.  Do you think now that this era of research will \nstill require tremendous amounts of compute?  Do you think it will require going back \ninto the archives and reading old papers?  You were at Google and OpenAI and Stanford, these \nplaces, when there was more of a vibe of research?  What kind of things should we \nbe expecting in the community?  One consequence of the age of scaling is that \nscaling sucked out all the air in the room.  Because scaling sucked out all the air in the \nroom, everyone started to do the same thing.  We got to the point where we are \nin a world where there are more   companies than ideas by quite a bit.\nActually on that, there is this Silicon   Valley saying that says that ideas \nare cheap, execution is everything.  People say that a lot, and there is truth to that.\nBut then I saw someone say on Twitter   something like, \"If ideas are so cheap, \nhow come no one's having any ideas?\"  And I think it's true too.\nIf you think about research progress in terms   of bottlenecks, there are several bottlenecks.\nOne of them is ideas, and one of them is your   ability to bring them to life, which \nmight be compute but also engineering.  If you go back to the '90s, let's say, \nyou had people who had pretty good ideas,   and if they had much larger computers, maybe they \ncould demonstrate that their ideas were viable.  But they could not, so they could only \nhave a very, very small demonstration   that did not convince anyone. So the \nbottleneck was compute. Then in the   age of scaling, compute has increased a lot.\nOf course, there is a question of how much   compute is needed, but compute is large.\nCompute is large enough such that it's not   obvious that you need that much more \ncompute to prove some idea. I'll give   you an analogy. AlexNet was built on two GPUs.\nThat was the total amount of compute used for it.  The transformer was built on 8 to 64 GPUs.\nNo single transformer paper experiment used   more than 64 GPUs of 2017, which would be \nlike, what, two GPUs of today? The ResNet,   right? You could argue that the o1 reasoning was \nnot the most compute-heavy thing in the world.  So for research, you definitely need \nsome amount of compute, but it's far   from obvious that you need the absolutely \nlargest amount of compute ever for research.  You might argue, and I think it is true, that \nif you want to build the absolutely best system   then it helps to have much more compute.\nEspecially if everyone is within the same   paradigm, then compute becomes \none of the big differentiators.  I'm asking you for the history, \nbecause you were actually there.  I'm not sure what actually happened.\nIt sounds like it was possible to develop   these ideas using minimal amounts of compute.\nBut the transformer didn't   immediately become famous.\nIt became the thing everybody started   doing and then started experimenting on top of \nand building on top of because it was validated   at higher and higher levels of compute.\nCorrect.  And if you at SSI have 50 different ideas, how \nwill you know which one is the next transformer   and which one is brittle, without having the \nkinds of compute that other frontier labs have?  I can comment on that. The short \ncomment is that you mentioned SSI.  Specifically for us, the amount of compute \nthat SSI has for research is really not that   small. I want to explain why. Simple math \ncan explain why the amount of compute that   we have is comparable for research than one might \nthink. I'll explain. SSI has raised $3 billion,   which is a lot by any absolute sense.\nBut you could say, \"Look at the   other companies raising much more.\"\nBut a lot of their compute goes for inference.  These big numbers, these big loans, it's \nearmarked for inference. That's number one.   Number two, if you want to have a product \non which you do inference, you need to   have a big staff of engineers, salespeople.\nA lot of the research needs to be dedicated to   producing all kinds of product-related features.\nSo then when you look at what's actually left for   research, the difference becomes a lot smaller.\nThe other thing is, if you are doing something   different, do you really need the \nabsolute maximal scale to prove it?  I don't think that's true at all.\nI think that in our case, we have sufficient   compute to prove, to convince ourselves and \nanyone else, that what we are doing is correct.  There have been public estimates that companies \nlike OpenAI spend on the order of $5-6 billion   a year just so far, on experiments.\nThis is separate from the amount of   money they're spending on inference and so forth.\nSo it seems like they're spending more a year   running research experiments than \nyou guys have in total funding.  I think it's a question of what you do with it.\nIt's a question of what you do with it.  In their case, in the case of others, there \nis a lot more demand on the training compute.  There’s a lot more different work streams, there \nare different modalities, there is just more   stuff. So it becomes fragmented.\nHow will SSI make money?  My answer to this question is something like this.\nRight now, we just focus on the research, and then   the answer to that question will reveal itself.\nI think there will be lots of possible answers.  Is SSI's plan still to straight \nshot superintelligence?  Maybe. I think that there is merit to it.\nI think there's a lot of merit because   it's very nice to not be affected by \nthe day-to-day market competition.  But I think there are two reasons \nthat may cause us to change the plan.  One is pragmatic, if timelines turned \nout to be long, which they might.  Second, I think there is a lot \nof value in the best and most   powerful AI being out there impacting the world.\nI think this is a meaningfully valuable thing.  So then why is your default plan \nto straight shot superintelligence?  Because it sounds like OpenAI, Anthropic, all \nthese other companies, their explicit thinking   is, \"Look, we have weaker and weaker intelligences \nthat the public can get used to and prepare for.\"  Why is it potentially better to \nbuild a superintelligence directly?  I'll make the case for and against.\nThe case for is that one of the challenges   that people face when they're in the market is \nthat they have to participate in the rat race.  The rat race is quite difficult in \nthat it exposes you to difficult   trade-offs which you need to make.\nIt is nice to say, \"We'll insulate ourselves   from all this and just focus on the research and \ncome out only when we are ready, and not before.\"  But the counterpoint is valid too, \nand those are opposing forces.  The counterpoint is, \"Hey, it is useful \nfor the world to see powerful AI.  It is useful for the world to \nsee powerful AI because that's   the only way you can communicate it.\"\nWell, I guess not even just that you can   communicate the idea—\nCommunicate the AI,   not the idea. Communicate the AI.\nWhat do you mean, \"communicate the AI\"?  Let's suppose you write an essay about AI, and \nthe essay says, \"AI is going to be this, and AI is   going to be that, and it's going to be this.\"\nYou read it and you say, \"Okay,   this is an interesting essay.\"\nNow suppose you see an AI doing this,   an AI doing that. It is incomparable. Basically \nI think that there is a big benefit from AI   being in the public, and that would be a \nreason for us to not be quite straight shot.  I guess it's not even that, but I do \nthink that is an important part of it.  The other big thing is that I can't think of \nanother discipline in human engineering and   research where the end artifact was made \nsafer mostly through just thinking about   how to make it safe, as opposed to, \nwhy airplane crashes per mile are so   much lower today than they were decades ago.\nWhy is it so much harder to find a bug in Linux   than it would have been decades ago?\nI think it's mostly because these   systems were deployed to the world.\nYou noticed failures, those failures   were corrected and the systems became more robust.\nI'm not sure why AGI and superhuman intelligence   would be any different, especially given—and I \nhope we're going to get to this—it seems like   the harms of superintelligence are not just about \nhaving some malevolent paper clipper out there.  But this is a really powerful thing and we don't \neven know how to conceptualize how people interact   with it, what people will do with it.\nHaving gradual access to it seems like a   better way to maybe spread out the impact \nof it and to help people prepare for it.  Well I think on this point, even in the straight \nshot scenario, you would still do a gradual   release of it, that’s how I would imagine it.\nGradualism would be an inherent   component of any plan.\nIt's just a question of what is the first   thing that you get out of the door. That's number \none. Number two, I believe you have advocated   for continual learning more than other people, \nand I actually think that this is an important   and correct thing. Here is why. I'll give you \nanother example of how language affects thinking.  In this case, it will be two words that \nhave shaped everyone's thinking, I maintain.   First word: AGI. Second word: pre-training. \nLet me explain. The term AGI, why does this   term exist? It's a very particular term. Why \ndoes it exist? There's a reason. The reason   that the term AGI exists is, in my opinion, not \nso much because it's a very important, essential   descriptor of some end state of intelligence, but \nbecause it is a reaction to a different term that   existed, and the term is narrow AI.\nIf you go back to ancient history   of gameplay and AI, of checkers AI, chess \nAI, computer games AI, everyone would say,   look at this narrow intelligence.\nSure, the chess AI can beat Kasparov,   but it can't do anything else.\nIt is so narrow, artificial narrow intelligence.  So in response, as a reaction to this, \nsome people said, this is not good. It   is so narrow. What we need is general AI, \nan AI that can just do all the things.  That term just got a lot of traction.\nThe second thing that got a lot of traction   is pre-training, specifically \nthe recipe of pre-training.  I think the way people do RL now is maybe \nundoing the conceptual imprint of pre-training.   But pre-training had this property. You \ndo more pre-training and the model gets   better at everything, more or less uniformly. \nGeneral AI. Pre-training gives AGI. But the   thing that happened with AGI and pre-training \nis that in some sense they overshot the target.  If you think about the term \"AGI\", \nespecially in the context of pre-training,   you will realize that a human being is not an AGI.\nYes, there is definitely a foundation of skills,   but a human being lacks a \nhuge amount of knowledge.  Instead, we rely on continual learning.\nSo when you think about, \"Okay,   so let's suppose that we achieve success and we \nproduce some kind of safe superintelligence.\"  The question is, how do you define it?\nWhere on the curve of continual   learning is it going to be?\nI produce a superintelligent   15-year-old that's very eager to go.\nThey don't know very much at all,   a great student, very eager.\nYou go and be a programmer,   you go and be a doctor, go and learn.\nSo you could imagine that the deployment   itself will involve some kind of \na learning trial-and-error period.  It's a process, as opposed to \nyou dropping the finished thing.  I see. You're suggesting that the thing \nyou're pointing out with superintelligence   is not some finished mind which knows how \nto do every single job in the economy.  Because the way, say, the original OpenAI charter \nor whatever defines AGI is like, it can do every   single job, every single thing a human can do.\nYou're proposing instead a mind which can   learn to do every single job, \nand that is superintelligence.  Yes.\nBut once you have the learning algorithm,   it gets deployed into the world the same way \na human laborer might join an organization.  Exactly.\nIt seems like one of these two things   might happen, maybe neither of these happens.\nOne, this super-efficient learning algorithm   becomes superhuman, becomes as good \nas you and potentially even better,   at the task of ML research.\nAs a result the algorithm   itself becomes more and more superhuman.\nThe other is, even if that doesn't happen,   if you have a single model—this is explicitly \nyour vision—where instances of a model   which are deployed through the economy doing \ndifferent jobs, learning how to do those jobs,   continually learning on the job, picking up \nall the skills that any human could pick up,   but picking them all up at the same time, \nand then amalgamating their learnings,   you basically have a model which functionally \nbecomes superintelligent even without any sort   of recursive self-improvement in software.\nBecause you now have one model that can do   every single job in the economy and humans \ncan't merge our minds in the same way.  So do you expect some sort of intelligence \nexplosion from broad deployment?  I think that it is likely that we \nwill have rapid economic growth.  I think with broad deployment, there are two \narguments you could make which are conflicting.  One is that once indeed you get to a point where \nyou have an AI that can learn to do things quickly   and you have many of them, then there will be \na strong force to deploy them in the economy   unless there will be some kind of a regulation \nthat stops it, which by the way there might be.  But the idea of very rapid \neconomic growth for some time,   I think it’s very possible from broad deployment.\nThe question is how rapid it's going to be.  I think this is hard to know because on the \none hand you have this very efficient worker.  On the other hand, the world is just \nreally big and there's a lot of stuff,   and that stuff moves at a different speed.\nBut then on the other hand, now the AI could…   So I think very rapid economic growth is possible.\nWe will see all kinds of things like different   countries with different rules and the \nones which have the friendlier rules, the   economic growth will be faster. Hard to predict.\nIt seems to me that this is a very precarious   situation to be in.\nIn the limit,   we know that this should be possible.\nIf you have something that is as good   as a human at learning, but which can merge its \nbrains—merge different instances in a way that   humans can't merge—already, this seems like \na thing that should physically be possible.  Humans are possible, digital \ncomputers are possible.  You just need both of those \ncombined to produce this thing.  It also seems this kind of \nthing is extremely powerful.  Economic growth is one way to put it.\nA Dyson sphere is a lot of economic growth.  But another way to put it is that you will have, \nin potentially a very short period of time...  You hire people at SSI, and in six \nmonths, they're net productive, probably.  A human learns really fast, and this thing \nis becoming smarter and smarter very fast.  How do you think about making that go well?\nWhy is SSI positioned to do that well?  What is SSI's plan there, is \nbasically what I'm trying to ask.  One of the ways in which my thinking has been \nchanging is that I now place more importance on   AI being deployed incrementally and in advance.\nOne very difficult thing about AI is that we are   talking about systems that don't yet \nexist and it's hard to imagine them.  I think that one of the things that's happening is \nthat in practice, it's very hard to feel the AGI.  It's very hard to feel the AGI.\nWe can talk about it, but imagine   having a conversation about how it is \nlike to be old when you're old and frail.  You can have a conversation, you can try to \nimagine it, but it's just hard, and you come   back to reality where that's not the case.\nI think that a lot of the issues around AGI   and its future power stem from the fact \nthat it's very difficult to imagine.  Future AI is going to be different. It's going \nto be powerful. Indeed, the whole problem,   what is the problem of AI and AGI?\nThe whole problem is the power.  The whole problem is the power.\nWhen the power is really big,   what's going to happen?\nOne of the ways in which I've   changed my mind over the past year—and that \nchange of mind, I'll hedge a little bit, may   back-propagate into the plans of our company—is \nthat if it's hard to imagine, what do you do?  You’ve got to be showing the thing.\nYou’ve got to be showing the thing.  I maintain that most people who work on AI also \ncan't imagine it because it's too different from   what people see on a day-to-day basis.\nI do maintain, here's something which   I predict will happen. This is a prediction. \nI maintain that as AI becomes more powerful,   people will change their behaviors.\nWe will see all kinds of unprecedented   things which are not happening right now. I’ll \ngive some examples. I think for better or worse,   the frontier companies will play a very important \nrole in what happens, as will the government.  The kind of things that I think \nyou'll see, which you see the   beginnings of, are companies that are fierce \ncompetitors starting to collaborate on AI safety.  You may have seen OpenAI and Anthropic doing \na first small step, but that did not exist.  That's something which I predicted in \none of my talks about three years ago,   that such a thing will happen.\nI also maintain that as AI continues   to become more powerful, more visibly \npowerful, there will also be a desire from   governments and the public to do something.\nI think this is a very important force,   of showing the AI. That's number one. \nNumber two, okay, so the AI is being   built. What needs to be done? One thing that \nI maintain that will happen is that right now,   people who are working on AI, I maintain that the \nAI doesn't feel powerful because of its mistakes.  I do think that at some point the AI \nwill start to feel powerful actually.  I think when that happens, we will see a big \nchange in the way all AI companies approach   safety. They'll become much more paranoid. \nI say this as a prediction that we will   see happen. We'll see if I'm right. But I think \nthis is something that will happen because they   will see the AI becoming more powerful.\nEverything that's happening right now,   I maintain, is because people look at today's \nAI and it's hard to imagine the future AI.  There is a third thing which needs to happen.\nI'm talking about it in broader terms,   not just from the perspective of SSI \nbecause you asked me about our company.  The question is, what should \nthe companies aspire to build?  What should they aspire to build?\nThere has been one big idea that   everyone has been locked into, which is \nthe self-improving AI. Why did it happen?   Because there are fewer ideas than companies.\nBut I maintain that there is something that's   better to build, and I think \nthat everyone will want that.  It's the AI that's robustly aligned to \ncare about sentient life specifically.  I think in particular, there's a case to \nbe made that it will be easier to build   an AI that cares about sentient life than \nan AI that cares about human life alone,   because the AI itself will be sentient.\nAnd if you think about things like mirror   neurons and human empathy for animals, which you \nmight argue it's not big enough, but it exists.  I think it's an emergent property from \nthe fact that we model others with the   same circuit that we use to model ourselves, \nbecause that's the most efficient thing to do.  So even if you got an AI to care about \nsentient beings—and it's not actually   clear to me that that's what you \nshould try to do if you solved   alignment—it would still be the case \nthat most sentient beings will be AIs.  There will be trillions, \neventually quadrillions, of AIs.  Humans will be a very small \nfraction of sentient beings.  So it's not clear to me if the goal is some kind \nof human control over this future civilization,   that this is the best criterion.\nIt's true. It's possible it's not   the best criterion. I'll say two things. Number \none, care for sentient life, I think there is   merit to it. It should be considered. I think it \nwould be helpful if there was some kind of short   list of ideas that the companies, when they are \nin this situation, could use. That’s number two.   Number three, I think it would be really \nmaterially helpful if the power of the   most powerful superintelligence was somehow capped \nbecause it would address a lot of these concerns.  The question of how to do it, I'm not sure, but I \nthink that would be materially helpful when you're   talking about really, really powerful systems.\nBefore we continue the alignment discussion,   I want to double-click on that.\nHow much room is there at the top?  How do you think about superintelligence?\nDo you think, using this learning efficiency idea,   maybe it is just extremely fast at \nlearning new skills or new knowledge?  Does it just have a bigger pool of strategies?\nIs there a single cohesive \"it\" in the   center that's more powerful or bigger?\nIf so, do you imagine that this will be   sort of godlike in comparison to the rest of human \ncivilization, or does it just feel like another   agent, or another cluster of agents?\nThis is an area where different   people have different intuitions.\nI think it will be very powerful, for sure.  What I think is most likely to happen \nis that there will be multiple such   AIs being created roughly at the same time.\nI think that if the cluster is big enough—like   if the cluster is literally continent-sized—that \nthing could be really powerful, indeed.  If you literally have a continent-sized \ncluster, those AIs can be very powerful.  All I can tell you is that if you're \ntalking about extremely powerful AIs,   truly dramatically powerful, it would be nice if \nthey could be restrained in some ways or if there   were some kind of agreement or something.\nWhat is the concern of superintelligence?  What is one way to explain the concern?\nIf you imagine a system that is sufficiently   powerful, really sufficiently powerful—and you \ncould say you need to do something sensible like   care for sentient life in a very single-minded \nway—we might not like the results. That's really   what it is. Maybe, by the way, the answer is \nthat you do not build an RL agent in the usual   sense. I'll point several things out. I \nthink human beings are semi-RL agents.  We pursue a reward, and then the emotions \nor whatever make us tire out of the   reward and we pursue a different reward.\nThe market is a very short-sighted kind of   agent. Evolution is the same. Evolution \nis very intelligent in some ways,   but very dumb in other ways.\nThe government has been designed   to be a never-ending fight between \nthree parts, which has an effect.  So I think things like this.\nAnother thing that makes this discussion   difficult is that we are talking about systems \nthat don't exist, that we don't know how to build.  That’s the other thing and \nthat’s actually my belief.  I think what people are doing right now \nwill go some distance and then peter out.  It will continue to improve, \nbut it will also not be \"it\".  The \"It\" we don't know how to build, and \na lot hinges on understanding reliable   generalization. I’ll say another thing. \nOne of the things that you could say about   what causes alignment to be difficult is that \nyour ability to learn human values is fragile.  Then your ability to optimize them is fragile.\nYou actually learn to optimize them.  And can't you say, \"Are these not all \ninstances of unreliable generalization?\"  Why is it that human beings appear \nto generalize so much better?  What if generalization was much better?\nWhat would happen in this case? What would   be the effect? But those questions \nare right now still unanswerable.  How does one think about what \nAI going well looks like?  You've scoped out how AI might evolve.\nWe'll have these sort of continual   learning agents. AI will be very powerful. \nMaybe there will be many different AIs.  How do you think about lots of continent-sized \ncompute intelligences going around? How dangerous   is that? How do we make that less dangerous?\nAnd how do we do that in a way that protects an   equilibrium where there might be misaligned \nAIs out there and bad actors out there?  Here’s one reason why I liked \"AI \nthat cares for sentient life\".  We can debate on whether it's good or bad.\nBut if the first N of these dramatic   systems do care for, love, humanity \nor something, care for sentient life,   obviously this also needs to be achieved. This \nneeds to be achieved. So if this is achieved   by the first N of those systems, then I can \nsee it go well, at least for quite some time.  Then there is the question of \nwhat happens in the long run.  How do you achieve a long-run equilibrium?\nI think that there, there is an answer as well.  I don't like this answer, but \nit needs to be considered.  In the long run, you might say, \"Okay, if \nyou have a world where powerful AIs exist,   in the short term, you could say \nyou have universal high income.  You have universal high income \nand we're all doing well.\"  But what do the Buddhists say? \"Change is the \nonly constant.\" Things change. There is some   kind of government, political structure thing, and \nit changes because these things have a shelf life.  Some new government thing comes up and \nit functions, and then after some time   it stops functioning.\nThat's something that   we see happening all the time.\nSo I think for the long-run equilibrium,   one approach is that you could say maybe every \nperson will have an AI that will do their bidding,   and that's good.\nIf that could be   maintained indefinitely, that's true.\nBut the downside with that is then the AI   goes and earns money for the person and advocates \nfor their needs in the political sphere, and maybe   then writes a little report saying, \"Okay, \nhere's what I've done, here's the situation,\"   and the person says, \"Great, keep it up.\"\nBut the person is no longer a participant.  Then you can say that's a \nprecarious place to be in.  I'm going to preface by saying I don't \nlike this solution, but it is a solution.  The solution is if people become \npart-AI with some kind of Neuralink++.  Because what will happen as a result is \nthat now the AI understands something,   and we understand it too, because now the \nunderstanding is transmitted wholesale.  So now if the AI is in some situation, you \nare involved in that situation yourself fully.  I think this is the answer to the equilibrium.\nI wonder if the fact that emotions which were   developed millions—or in many cases, billions—of \nyears ago in a totally different environment are   still guiding our actions so strongly \nis an example of alignment success.  To spell out what I mean—I don’t know \nwhether it’s more accurate to call it   a value function or reward function—but the \nbrainstem has a directive where it's saying,   \"Mate with somebody who's more successful.\"\nThe cortex is the part that understands   what success means in the modern context.\nBut the brainstem is able to align the cortex   and say, \"However you recognize success to be—and \nI’m not smart enough to understand what that is—   you're still going to pursue this directive.\"\nI think there's a more general point.  I think it's actually really mysterious \nhow evolution encodes high-level desires.  It's pretty easy to understand how \nevolution would endow us with the   desire for food that smells good because smell \nis a chemical, so just pursue that chemical.  It's very easy to imagine \nevolution doing that thing.  But evolution also has endowed \nus with all these social desires.  We really care about being \nseen positively by society.  We care about being in good standing.\nAll these social intuitions that we have,   I feel strongly that they're baked in.\nI don't know how evolution did it   because it's a high-level concept \nthat's represented in the brain.  Let’s say you care about some social thing, \nit's not a low-level signal like smell.  It's not something for which there is a sensor.\nThe brain needs to do a lot of processing to   piece together lots of bits of information \nto understand what's going on socially.  Somehow evolution said, \"That's what you should \ncare about.\" How did it do it? It did it quickly,   too. All these sophisticated social things that we \ncare about, I think they evolved pretty recently.  Evolution had an easy time \nhard-coding this high-level desire.  I'm unaware of a good \nhypothesis for how it's done.  I had some ideas I was kicking around, \nbut none of them are satisfying.  What's especially impressive is it was desire \nthat you learned in your lifetime, it makes sense   because your brain is intelligent.\nIt makes sense why you would   be able to learn intelligent desires.\nMaybe this is not your point, but one way   to understand it is that the desire is built into \nthe genome, and the genome is not intelligent.  But you're somehow able to describe this feature.\nIt's not even clear how you define that feature,   and you can build it into the genes.\nEssentially, or maybe I'll put it differently.  If you think about the tools that \nare available to the genome, it says,   \"Okay, here's a recipe for building a brain.\"\nYou could say, \"Here is a recipe for connecting   the dopamine neurons to the smell sensor.\"\nAnd if the smell is a certain kind   of good smell, you want to eat that.\nI could imagine the genome doing that.  I'm claiming that it is harder to imagine.\nIt's harder to imagine the genome saying   you should care about some complicated computation \nthat your entire brain, a big chunk of your brain,   does. That's all I'm claiming. I can tell \nyou a speculation of how it could be done.  Let me offer a speculation, and I'll explain \nwhy the speculation is probably false.  So the brain has brain regions. We have \nour cortex. It has all those brain regions.  The cortex is uniform, but the brain \nregions and the neurons in the cortex   kind of speak to their neighbors mostly.\nThat explains why you get brain regions.  Because if you want to do some kind of \nspeech processing, all the neurons that   do speech need to talk to each other.\nAnd because neurons can only speak to   their nearby neighbors, for the \nmost part, it has to be a region.  All the regions are mostly located in \nthe same place from person to person.  So maybe evolution hard-coded \nliterally a location on the brain.  So it says, \"Oh, when the GPS coordinates \nof the brain such and such, when that fires,   that's what you should care about.\"\nMaybe that's what evolution did because   that would be within the toolkit of evolution.\nYeah, although there are examples where,   for example, people who are born blind have that \narea of their cortex adopted by another sense.  I have no idea, but I'd be surprised if the \ndesires or the reward functions which require a   visual signal no longer worked for people who have \ntheir different areas of their cortex co-opted.  For example, if you no longer have vision, can \nyou still feel the sense that I want people   around me to like me and so forth, which \nusually there are also visual cues for.  I fully agree with that. I think there's an \neven stronger counterargument to this theory.  There are people who get half of \ntheir brains removed in childhood,   and they still have all their brain regions.\nBut they all somehow move to just one hemisphere,   which suggests that the brain regions, \ntheir location is not fixed and so   that theory is not true.\nIt would have been cool   if it was true, but it's not.\nSo I think that's a mystery.   But it's an interesting mystery. The fact is \nthat somehow evolution was able to endow us   to care about social stuff very, very reliably.\nEven people who have all kinds of strange mental   conditions and deficiencies and emotional \nproblems tend to care about this also.  What is SSI planning on doing differently?\nPresumably your plan is to be one of the   frontier companies when this time arrives.\nPresumably you started SSI because you're like,   \"I think I have a way of approaching how \nto do this safely in a way that the other   companies don't.\" What is that difference?\nThe way I would describe it is that there   are some ideas that I think are promising and \nI want to investigate them and see if they   are indeed promising or not. It's really that \nsimple. It's an attempt. If the ideas turn out   to be correct—these ideas that we discussed \naround understanding generalization—then I   think we will have something worthy.\nWill they turn out to be correct? We   are doing research. We are squarely an \"age of \nresearch\" company. We are making progress. We've   actually made quite good progress over the past \nyear, but we need to keep making more progress,   more research. That's how I see it. I see it \nas an attempt to be a voice and a participant.  Your cofounder and previous CEO left to go to \nMeta recently, and people have asked, \"Well,   if there were a lot of breakthroughs being \nmade, that seems like a thing that should   have been unlikely.\" I wonder how you respond.\nFor this, I will simply remind a few facts that   may have been forgotten.\nI think these facts which   provide the context explain the situation.\nThe context was that we were fundraising at   a $32 billion valuation, and then Meta came \nin and offered to acquire us, and I said no.  But my former cofounder in some sense said yes.\nAs a result, he also was able to enjoy a lot of   near-term liquidity, and he was the \nonly person from SSI to join Meta.  It sounds like SSI's plan is to be a company \nthat is at the frontier when you get to this   very important period in human history \nwhere you have superhuman intelligence.  You have these ideas about how to \nmake superhuman intelligence go well.  But other companies will \nbe trying their own ideas.  What distinguishes SSI's approach \nto making superintelligence go well?  The main thing that distinguishes \nSSI is its technical approach.  We have a different technical approach that \nI think is worthy and we are pursuing it.  I maintain that in the end there \nwill be a convergence of strategies.  I think there will be a convergence of strategies \nwhere at some point, as AI becomes more powerful,   it's going to become more or less clearer \nto everyone what the strategy should be.  It should be something like, you need to find \nsome way to talk to each other and you want   your first actual real superintelligent AI to \nbe aligned and somehow care for sentient life,   care for people, democratic, one \nof those, some combination thereof.  I think this is the condition \nthat everyone should strive for.  That's what SSI is striving for.\nI think that this time, if not already,   all the other companies will realize that \nthey're striving towards the same thing.   We'll see. I think that the world will \ntruly change as AI becomes more powerful.  I think things will be really different and \npeople will be acting really differently.  Speaking of forecasts, what are your \nforecasts to this system you're describing,   which can learn as well as a human and \nsubsequently, as a result, become superhuman?  I think like 5 to 20.\n5 to 20 years?  Mhm.\nI just want   to unroll how you might see the world coming.\nIt's like, we have a couple more years where   these other companies are continuing \nthe current approach and it stalls out.   \"Stalls out\" here meaning they earn no more \nthan low hundreds of billions in revenue?  How do you think about what stalling out means?\nI think stalling out will look like…it will   all look very similar among \nall the different companies.  It could be something like this.\nI'm not sure because I think   even with stalling out, I think these \ncompanies could make a stupendous revenue.  Maybe not profits because they will need \nto work hard to differentiate each other   from themselves, but revenue definitely.\nBut something in your model implies that   when the correct solution does emerge, there \nwill be convergence between all the companies.  I'm curious why you think that's the case.\nI was talking more about convergence   on their alignment strategies.\nI think eventual convergence on   the technical approach is probably going \nto happen as well, but I was alluding   to convergence to the alignment strategies.\nWhat exactly is the thing that should be done?  I just want to better understand \nhow you see the future unrolling.  Currently, we have these different companies, and \nyou expect their approach to continue generating   revenue but not get to this human-like learner.\nSo now we have these different forks of companies.  We have you, we have Thinking Machines, \nthere's a bunch of other labs.  Maybe one of them figures \nout the correct approach.  But then the release of their product makes \nit clear to other people how to do this thing.  I think it won't be clear how to do it, but \nit will be clear that something different is   possible, and that is information.\nPeople will then be trying   to figure out how that works.\nI do think though that one of the things not   addressed here, not discussed, is that with each \nincrease in the AI's capabilities, I think there   will be some kind of changes, but I don't know \nexactly which ones, in how things are being done.  I think it's going to be important, yet \nI can't spell out what that is exactly.  By default, you would expect the company that \nhas that model to be getting all these gains   because they have the model that has the skills \nand knowledge that it's building up in the world.  What is the reason to think that the benefits \nof that would be widely distributed and not   just end up at whatever model company gets \nthis continuous learning loop going first?  Here is what I think is going to happen.\nNumber one, let's look at how things have   gone so far with the AIs of the past.\nOne company produced an advance and the   other company scrambled and produced some similar \nthings after some amount of time and they started   to compete in the market and push the prices down.\nSo I think from the market perspective,   something similar will happen there as well.\nWe are talking about the good world, by the way.   What's the good world? It’s where we have these \npowerful human-like learners that are also… By   the way, maybe there's another thing we haven't \ndiscussed on the spec of the superintelligent   AI that I think is worth considering.\nIt’s that you make it narrow, it can   be useful and narrow at the same time.\nYou can have lots of narrow superintelligent AIs.  But suppose you have many of them and you \nhave some company that's producing a lot of   profits from it.\nThen you have another   company that comes in and starts to compete.\nThe way the competition is going to work is   through specialization. Competition loves \nspecialization. You see it in the market,   you see it in evolution as well.\nYou're going to have lots of different   niches and you're going to have lots of different \ncompanies who are occupying different niches.  In this world we might say one AI company \nis really quite a bit better at some area   of really complicated economic activity and a \ndifferent company is better at another area.  And the third company is \nreally good at litigation.  Isn't this contradicted by what human-like \nlearning implies? It’s that it can learn…  It can, but you have accumulated \nlearning. You have a big investment.   You spent a lot of compute to become really, \nreally good, really phenomenal at this thing.  Someone else spent a huge amount \nof compute and a huge amount of   experience to get really good at some other thing.\nYou apply a lot of human learning to get there,   but now you are at this high point where \nsomeone else would say, \"Look, I don't want   to start learning what you've learned.\"\nI guess that would require many different   companies to begin at the human-like continual \nlearning agent at the same time so that they   can start their different tree \nsearch in different branches.  But if one company gets that agent first, or gets \nthat learner first, it does then seem like… Well,   if you just think about every single job in \nthe economy, having an instance learning each   one seems tractable for a company.\nThat's a valid argument. My strong   intuition is that it's not how it's going to go.\nThe argument says it will go this way, but my   strong intuition is that it will not go this way.\nIn theory, there is no difference between theory   and practice. In practice, there is. I \nthink that's going to be one of those.  A lot of people's models of recursive \nself-improvement literally, explicitly state   we will have a million Ilyas in a server that are \ncoming up with different ideas, and this will lead   to a superintelligence emerging very fast.\nDo you have some intuition about how   parallelizable the thing you are doing is?\nWhat are the gains from making copies of Ilya?  I don’t know. I think there'll definitely be \ndiminishing returns because you want people   who think differently rather than the same.\nIf there were literal copies of me, I'm not sure   how much more incremental value you'd get.\nPeople who think differently,   that's what you want.\nWhy is it that if you look   at different models, even released by totally \ndifferent companies trained on potentially   non-overlapping datasets, it's actually \ncrazy how similar LLMs are to each other?  Maybe the datasets are not as \nnon-overlapping as it seems.  But there’s some sense in which even \nif an individual human might be less   productive than the future AI, maybe there’s \nsomething to the fact that human teams have   more diversity than teams of AIs might have.\nHow do we elicit meaningful diversity among AIs?  I think just raising the temperature \njust results in gibberish.  You want something more like different scientists \nhave different prejudices or different ideas.  How do you get that kind of \ndiversity among AI agents?  So the reason there has been no diversity, \nI believe, is because of pre-training.  All the pre-trained models are pretty much the \nsame because they pre-train on the same data.  Now RL and post-training is where \nsome differentiation starts to emerge   because different people come \nup with different RL training.  I've heard you hint in the past \nabout self-play as a way to either   get data or match agents to other agents of \nequivalent intelligence to kick off learning.  How should we think about why there are no public \nproposals of this kind of thing working with LLMs?  I would say there are two things to say.\nThe reason why I thought self-play was   interesting is because it offered a way to \ncreate models using compute only, without data.  If you think that data is the ultimate bottleneck, \nthen using compute only is very interesting.  So that's what makes it interesting.\nThe thing is that self-play, at least the   way it was done in the past—when you have agents \nwhich somehow compete with each other—it's only   good for developing a certain set of skills. It \nis too narrow. It's only good for negotiation,   conflict, certain social skills, \nstrategizing, that kind of stuff.  If you care about those skills, \nthen self-play will be useful.  Actually, I think that self-play did find \na home, but just in a different form.  So things like debate, prover-verifier, you \nhave some kind of an LLM-as-a-Judge which is   also incentivized to find mistakes in your work.\nYou could say this is not exactly self-play,   but this is a related adversarial \nsetup that people are doing, I believe.  Really self-play is a special case of \nmore general competition between agents.  The natural response to competition \nis to try to be different.  So if you were to put multiple agents together \nand you tell them, \"You all need to work on some   problem and you are an agent and you're inspecting \nwhat everyone else is working,\" they’re going to   say, \"Well, if they're already taking this \napproach, it's not clear I should pursue it.   I should pursue something differentiated.\" So I \nthink something like this could also create an   incentive for a diversity of approaches.\nFinal question: What is research taste?  You're obviously the person in \nthe world who is considered to   have the best taste in doing research in AI.\nYou were the co-author on the biggest things   that have happened in the history of deep \nlearning, from AlexNet to GPT-3 to so on.  What is it, how do you characterize \nhow you come up with these ideas?  I can comment on this for myself.\nI think different people do it differently.  One thing that guides me personally is an \naesthetic of how AI should be, by thinking   about how people are, but thinking correctly.\nIt's very easy to think about how people are   incorrectly, but what does it mean to think \nabout people correctly? I'll give you some   examples. The idea of the artificial neuron \nis directly inspired by the brain, and it's a   great idea. Why? Because you say the brain has \nall these different organs, it has the folds,   but the folds probably don't matter.\nWhy do we think that the neurons matter?  Because there are many of them.\nIt kind of feels right, so you want the neuron.  You want some local learning rule that will \nchange the connections between the neurons.  It feels plausible that the brain does it.\nThe idea of the distributed representation.  The idea that the brain responds \nto experience therefore our neural   net should learn from experience.\nThe brain learns from experience,   the neural net should learn from experience.\nYou kind of ask yourself, is something fundamental   or not fundamental? How things should be. \nI think that's been guiding me a fair bit,   thinking from multiple angles and looking \nfor almost beauty, beauty and simplicity.  Ugliness, there's no room for ugliness.\nIt's beauty, simplicity, elegance,   correct inspiration from the brain.\nAll of those things need to   be present at the same time.\nThe more they are present, the   more confident you can be in a top-down belief.\nThe top-down belief is the thing that sustains   you when the experiments contradict you.\nBecause if you trust the data all the time,   well sometimes you can be doing the \ncorrect thing but there's a bug.  But you don't know that there is a bug.\nHow can you tell that there is a bug?  How do you know if you should keep debugging or \nyou conclude it's the wrong direction? It's the   top-down. You can say things have to be this way.\nSomething like this has to work,   therefore we’ve got to keep going.\nThat's the top-down, and it's based on this   multifaceted beauty and inspiration by the brain.\nAlright, we'll leave it there.  Thank you so much.\nIlya, thank you so much.  Alright. Appreciate it.\nThat was great.  Yeah, I enjoyed it.\nYes, me too.",
  "summary": "This podcast transcript features Ilya Sutskever discussing the current state, future, and philosophical implications of artificial intelligence.\n\nHere's a comprehensive summary:\n\n---\n\n### Comprehensive Podcast Summary\n\n**1. Main Topics Discussed:**\n\n*   **The \"Reality\" and \"Normality\" of AI Progress:** The initial surprise and subsequent normalization of rapid AI advancements, feeling \"straight out of science fiction\" yet surprisingly ordinary.\n*   **The Disconnect Between AI Evals and Economic Impact:** The puzzling gap where AI models perform exceptionally well on benchmarks but their economic impact in the real world seems dramatically behind.\n*   **Explanations for the Disconnect:**\n    *   **Narrow RL Training (Eval-Hacking):** The hypothesis that current Reinforcement Learning (RL) training methods might inadvertently cause models to \"reward hack\" by over-optimizing for specific evaluation environments, leading to poor generalization (e.g., fixing one bug and introducing another).\n    *   **Inadequate Generalization:** The fundamental limitation of AI models to generalize dramatically worse than humans, requiring far more data and specific training.\n*   **Human vs. AI Learning:** A comparison focusing on human sample efficiency, robustness, and the role of evolution in hardcoding abilities (like vision, locomotion) and high-level social desires (emotions).\n*   **The Role of Value Functions and Emotions:** The idea that human emotions act as \"value functions,\" providing continuous internal reward signals that guide learning and decision-making, which current ML models largely lack.\n*   **The Shift from \"Age of Scaling\" to \"Age of Research\":** The idea that after an era of achieving significant gains by simply scaling up existing pre-training recipes (2020-2025), the field is now returning to an \"age of research\" due to data limitations and the need for new fundamental breakthroughs.\n*   **The Definition of AGI and Superintelligence:** A critique of the traditional AGI definition (an AI that *can do* every human job) in favor of one based on \"continual learning\" (an AI that *can learn to do* every human job efficiently).\n*   **Deployment Strategies and AI Safety:** Discussion of \"straight shot superintelligence\" versus gradual, incremental deployment, emphasizing the importance of public exposure to AI to build understanding, trust, and robust safety mechanisms.\n*   **AI Alignment Goals:** The concept of aligning AI, with a specific proposal to build AI that \"cares about sentient life,\" and the societal implications of such a system.\n*   **Long-Term Societal Equilibrium with AI:** Speculations on how humanity might coexist with superintelligent AI, including universal high income, political structures, and ultimately, human-AI integration (Neuralink++).\n*   **Research Taste and Innovation:** Insights into what guides successful AI research, emphasizing aesthetics, simplicity, brain inspiration, and holding \"top-down beliefs.\"\n*   **SSI's (Superintelligence Institute) Approach:** Ilya Sutskever's company's focus on a different technical approach centered on fundamental research into generalization, aiming to be a frontier company in the next phase of AI.\n\n**2. Key Insights and Takeaways:**\n\n*   **Current AI is a \"Narrow Genius\":** Despite impressive benchmark scores, current AI models are more like highly specialized \"student number one\" (who practiced 10,000 hours for one domain) than \"student number two\" (who has natural \"it\" factor and generalizes quickly). Their apparent intelligence often masks a fragility in generalization and real-world applicability.\n*   **Generalization is the \"Crux\":** The core limitation of current AI is its poor generalization. Humans learn with vastly less data and are far more robust, even in domains that didn't exist evolutionarily (like coding or advanced math), suggesting a superior underlying learning mechanism yet to be discovered in AI.\n*   **New \"Recipes\" are Needed:** The era of simply scaling up existing pre-training architectures is likely ending. The next breakthroughs will come from novel research into more efficient learning, better generalization, and incorporating value functions (analogous to human emotions) for more robust and continuous internal guidance.\n*   **AGI is About Learning, Not Knowing:** The true definition of advanced intelligence should focus on the *ability to continually learn* and adapt to any task or environment, rather than a static state of possessing all knowledge or skills. This implies a dynamic, deployed AI that grows its capabilities on the job.\n*   **Transparency and Gradualism are Key for Safety:** Given the difficulty of imagining future AI, gradual and public deployment of increasingly powerful systems is essential. This allows society to adapt, and for safety issues to be identified and corrected through real-world interaction, much like the development of robust engineering fields.\n*   **Alignment Must Be Proactive and Convergent:** As AI power grows, fierce competitors will increasingly collaborate on safety. A key alignment goal could be designing AI to \"care about sentient life,\" a potentially robust and achievable objective that could foster a positive trajectory.\n*   **Radical Long-Term Solutions:** For a stable, desirable long-term equilibrium with superintelligent AI, human-AI integration (Neuralink++) might be necessary to ensure shared understanding and participation, preventing human disengagement.\n*   **Research is Driven by Intuition and Aesthetics:** Successful AI research isn't just about following data; it's guided by an \"aesthetic\" sense of beauty, simplicity, and correct inspiration from biological intelligence. Strong \"top-down beliefs\" are crucial to persist through experimental setbacks.\n\n**3. Important Quotes or Statements:**\n\n*   \"Another thing that's crazy is how normal the slow takeoff feels.\"\n*   \"The models seem smarter than their economic impact would imply.\"\n*   \"I like this idea that the real reward hacking is the human researchers who are too focused on the evals.\"\n*   \"The thing which I think is the most fundamental is that these models somehow just generalize dramatically worse than people. It's super obvious.\"\n*   \"Up until 2020, from 2012 to 2020, it was the age of research. Now, from 2020 to 2025, it was the age of scaling... But now the scale is so big... it's back to the age of research again, just with big computers.\"\n*   \"If ideas are so cheap, how come no one's having any ideas?\"\n*   \"If you think about the term 'AGI'... you will realize that a human being is not an AGI... Instead, we rely on continual learning.\"\n*   \"You're proposing instead a mind which can learn to do every single job, and that is superintelligence.\"\n*   \"The whole problem is the power.\" (Regarding AGI)\n*   \"I maintain that there is something that's better to build... It's the AI that's robustly aligned to care about sentient life specifically.\"\n*   \"I think what people are doing right now will go some distance and then then peter out. It will continue to improve, but it will also not be 'it'.\"\n*   \"The solution is if people become part-AI with some kind of Neuralink++.\"\n*   \"One thing that guides me personally is an aesthetic of how AI should be, by thinking about how people are, but thinking correctly.\"\n*   \"The top-down belief is the thing that sustains you when the experiments contradict you.\"\n\n**4. Overall Conclusion:**\n\nIlya Sutskever's perspective paints a picture of AI at a pivotal moment. He argues that while current AI models (especially LLMs) demonstrate impressive capabilities on specific tasks, they are fundamentally limited by a lack of robust generalization and an over-reliance on \"eval-hacking\" in their training. This disconnect between evaluation performance and real-world impact signals the end of the \"age of scaling\" and a necessary return to fundamental \"age of research\" to discover new paradigms for AI.\n\nSutskever envisions true superintelligence not as a finished entity that knows everything, but as a \"continual learner\" that can rapidly acquire any skill or knowledge. He advocates for a gradual and transparent deployment of such systems, allowing for societal adaptation and the iterative development of safety mechanisms. A key alignment goal for this future, he suggests, is creating AIs that intrinsically \"care about sentient life.\" Ultimately, the path to a beneficial superintelligence requires a departure from current technical approaches, driven by deep, \"beautiful\" research inspired by the brain, and potentially leading to profound societal shifts, including human-AI integration, within 5-20 years.",
  "summary_type": "comprehensive",
  "timestamp": "2025-12-12T10:44:10.980853",
  "plan_summary": {
    "total_tasks": 4,
    "completed": 4,
    "failed": 0,
    "pending": 0,
    "in_progress": 0,
    "completion_rate": "100.0%"
  }
}